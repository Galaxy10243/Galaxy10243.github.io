[{"title":"Diffusion Policy:Visuomotor Policy Learning via Action Diffusion","url":"/2025/08/16/Diffusion-Policy/","content":"\n引言在机器人手臂操作方面一直存在诸多挑战。我们熟悉的工业场景中的组装机械臂，往往依赖于写死的程序指令进行控制，具有高度规范化与高精度的特点。而当机械臂需要在复杂、多变的环境中与外界进行交互时，就不得不涉及学习策略、协同性以及时间序列决策等难题。如今，在工业高精度场景下的机械臂已日渐成熟，精度也越来越高，但在更复杂的任务中，依然存在效率低下、动作连贯性不足以及学习策略不佳等问题（论文中对此有具体例子）。\n针对这些问题，Diffusion Policy 框架提出了一种全新的解决思路：它将机器人动作生成建模为条件扩散过程，在逐步去噪中得到连贯、合理的动作序列，从而有效改善动作的多模态建模能力、时序一致性和训练稳定性。实验证明，Diffusion Policy 在多项机器人任务中均显著超越了现有方法，展现出更强的学习能力与表现力。\nDiffusion 驱动下的机械手 展现了极强的学习能力与连贯性，而在 CoRL 座谈会 上，研究者们对其讨论也非常热烈。读完论文后，我也不得不感叹一句：Diffusion Policy is SOTA！\n1. 什么是 Diffusion Policy？Diffusion Policy 是一种基于 条件扩散模型（Conditional Denoising Diffusion Process）的机器人视觉-运动策略学习方法。\nDiffusion Policy其实解决的是一个机器人输出的问题，过往的很多工作大家都注重在解决输入的问题，但机器人最终要执行，我们的工作就在于解决机器人动作和输出的问题。更确切地说，我们的创新聚焦于机器人的动作端而非输入端，在输入端使用的是非常普通的东西。尽管输入端有很多可以提高的地方，但机器人学习方法必须注重输出端，先前的算法在输出端的表现都不够好。因此，无论输入端有多么创新，如果输出端表现不佳，就像”茶壶煮饺子倒不出”一样，将无法发挥潜力。它的基本思想是：\n\n不直接预测动作，而是从随机噪声出发，经过多步去噪，逐渐生成符合任务要求的动作序列；\n\n使用视觉观测作为条件，保证生成的动作与环境状态相匹配；\n\n生成的是高维动作序列，而非单步动作，能确保动作的时序一致性和连贯性。\n\n\n它的优势主要体现在：\n\n表达多模态分布：可以学到不同的合理操作方式；\n\n适配高维动作空间：能生成完整的动作序列，而不是单步预测；\n\n训练稳定：避免了能量模型训练中的负采样问题。\n\n\n\n\n2. 与传统方法区别2.1 解决了机器人Multi-Modal的问题简单理解，现实世界中解决某一特定任务的方式是多样的，而不是唯一的。但神经网络预测只能给出单一的方式，无法应对可能有多种方式的任务情况。\n什么是机器人Multi-Moda问题，假设我现在在开车，前面有一棵树。比如说，我雇佣了100个司机来解决这个问题。在这种情况下，有可能有50个司机选择往左拐，绕过树的左边，还有50个司机选择从树的右边绕过去。在这种情况下，往左绕和往右绕都是完全合理的。然而，当我们将所有这些解决方案合并为一个时，问题就变成了一个多模态分布，即我看到的相同场景有两种不同的选择。这对传统神经网络的预测来说并不友好，因为它通常使用均方误差（MSE）损失进行训练，而这无法有效处理Multi-Modal情况。\n为了解决这个问题，引入了概率分布，使得神经网络不再是一个输入一个输出的函数，而是一个输入可以有多个输出的函数。这种方法提供了更大的灵活性，可以表示各种概率分布，解决了原有方法的限制。\n2.2 解决了Action Space Scalabiltiy或者sequential correlation问题数据预测有两种方法：一是直接输出一个数值，另一种是将可能的数值分成几个区间，进行离散预测。在预测Multi-Modal Action的时候，人们倾向于采用离散预测，将连续值问题转化为分类问题，但这样做涉及的算力成本很高，尤其在处理高维空间时。此外，针对机器人控制问题，如果采用分类方法，每一步都需要预测下一步要执行的动作，而实际需求是希望一次性预测多步动作，这就涉及到了连续控制中的动作一致性问题。解决这个问题的挑战在于平衡成本和对高维连续控制的需求。\n假设我要控制的是一个具有六个自由度的机械手，甚至考虑到夹爪开关，有七个自由度，这时如果我要对其进行分类，就不再是在一个维度上切分成100份，而是每个维度都要切分成1000份。然后，将所有这些切分的部分相乘，才能得到我们整个空间的方法。如果采用这种方法，成本将会非常非常高。随着维度的增加，成本会呈指数级增长\n由于它们预测高维空间的成本非常高，因为它们只能预测一步，接下来的步骤是什么。如果再加上更多的步骤，维度就会变得越来越高，它们就无法胜任。然而，实际上我们现在追求的是具有以下特性的方法：不仅可以预测每一步，而且可以在高维连续控制中实现。对于我们来说，我们可以直接预测未来每一步，无论是接下来的20步还是100步，是向左还是向右，而不是在每一步预测之后再执行，再决定下一步该怎么走。\n2.3 解决了Training Stability问题Diffusion Policy和其他使用生成式模型(LSTM-GMM、BET、IBC)的策略比，他的最大特点是训练过程非常稳定。关于训练稳定性，迟宬的进一步解释是：在Robot Learning领域，机器人动作执行主要有三种方法：包括直接回归、分类预测和生成式模型。第一类回归，即将神经网络视为一个函数，输入是图片，输出是一个动作。这是最常见的方法，绝大多数强化学习都采用这种方式。然而，这种方法存在一些问题，正如之前提到的。第二类分类预测，这种方法通过预测分类来生成动作，前文已经大致描述，不再详细赘述。第三类生成模型，理论上所有的生成模型都可以预测连续的多模态分布，但很多生成模型的问题是训练不稳定。\n基于Diffusion Model的第三类方法具有一个重要的优势，即训练非常稳定。这也是为什么Diffusion Model当前在图像生成方面取得了成功，而当时的生成对抗网络（GAN）并没有成功的原因。在当时，GAN在学术界能够产生一些不错的效果，但当你真的将其应用于产品时，你会发现非常困难。要训练一个有效的GAN，你需要疯狂地调整参数，然后才能训练出可用的生成器。\n而Diffusion方法的强大之处在于，它的性能不逊色于GAN，但其训练过程非常稳定。基本上，你可以随便调整参数，生成器就能够输出结果，可能效果不是最优的，但基本上都能work。同时，这也解释了为什么像Stable Diffusion这样的方法，以及现在各种图像生成模型能够在如此庞大的数据集上进行训练，这是因为它们的训练非常稳定。如果你在如此大规模的数据上使用其他方法进行训练，可能会在训练一段时间后出现奇怪的问题，模型无法进一步优化。\n\n3. 模型架构梳理Diffusion Policy 的架构主要由三部分组成：\n3.1 视觉输入（Visual Encoder）使用 ResNet 作为基础结构；\n修改：全局平均池化替换为 空间 Softmax 池化；BatchNorm 改为 GroupNorm；\n输入：多视角图像和机器人自身状态；\n输出：观测特征 $O_t$。\n3.2 动作扩散（Action Diffusion）从随机噪声动作序列 $A_t^K$ 出发，逐步去噪得到最终动作 $A_t^0$；\n每次推理预测 $T_p$ 步动作，但只执行其中 $T_a$ 步，然后重新规划；\n保证动作的时序一致性，同时具有实时响应能力。\n3.3 Transformer 时序结构CNN 适合低频任务，但容易过度平滑；\nTransformer 更适合需要快速动作变化的任务；\n输入：带噪动作 embedding + 扩散步数 embedding；\n融合：通过跨注意力融合视觉特征 $O_t$；\n输出：噪声预测 $\\epsilon_\\theta$，逐步修正动作序列。\n\n4. 数学推导Diffusion Policy 的数学基础来源于 去噪扩散概率模型（DDPM）。它的关键思想是：从高斯噪声出发，通过逐步去噪的方式，生成符合分布的动作序列。\n4.1 扩散过程公式标准的 DDPM 去噪更新公式为：\n$$x_{k-1} &#x3D; \\alpha \\big(x_k - \\gamma , \\epsilon_\\theta(x_k, k)\\big) + \\mathcal{N}(0, \\sigma^2 I)$$\n其中：\n\n$x_k$：含噪动作；\n$\\epsilon_\\theta$：神经网络预测的噪声；\n$\\gamma$：学习率&#x2F;步长；\n$\\alpha, \\sigma$：噪声调度参数。\n\n4.2 梯度解释该过程等价于在能量函数 $E(x)$ 上执行带噪声的梯度下降：\n$$x’ &#x3D; x - \\gamma \\nabla E(x)$$\n因此 $\\epsilon_\\theta$ 可以理解为学习了能量函数的梯度场，保证采样的稳定性。\n4.3 条件扩散策略为了让生成动作与机器人观测相一致，Diffusion Policy 在扩散过程中引入观测 $O_t$，形成条件扩散：\n$$A_{t}^{k-1} &#x3D; \\alpha \\big(A_t^k - \\gamma , \\epsilon_\\theta(O_t, A_t^k, k)\\big) + \\mathcal{N}(0, \\sigma^2 I)$$\n其中：\n\n$A_t^k$：第 $k$ 步含噪动作序列；\n$O_t$：当前的视觉与状态观测。\n\n4.4 训练目标在训练中，Diffusion Policy 会随机选择一个扩散步数 $k$，并为真实动作 $A_t^0$ 添加噪声 $\\epsilon_k$，训练网络去预测这个噪声：\n$$L &#x3D; \\mathbb{E} \\big[ |\\epsilon_k - \\epsilon_\\theta(O_t, A_t^0 + \\epsilon_k, k)|^2 \\big]$$\n该损失函数本质上是一个 MSE（均方误差），它保证了模型在所有扩散步数上都能有效学习，从而得到稳定的训练过程。\n\n5. 与其他学习策略的比较5.1 显式策略 (Explicit Policy)\n直接从观测到动作的回归或分类。\n\n优点：实现简单、推理速度快。\n\n缺点：难以处理多模态动作分布，容易平均化导致动作模糊。\n\n\n5.2 隐式策略 (Implicit Policy, EBM)\n通过能量函数建模动作分布，选择能量最小的动作。\n\n优点：可以表达多模态分布。\n\n缺点：训练依赖负采样，往往不稳定；推理时开销较大。\n\n\n5.3 Diffusion Policy 的创新\n使用扩散模型迭代生成动作，自然支持多模态；\n\n动作序列预测：提升时序一致性，避免抖动和不连贯；\n\n视觉条件化：显著加快推理速度，适合实时控制；\n\n闭环控制：结合 receding horizon，让机器人既能长远规划又能及时响应。\n\n\n\n5.4 不足\n推理速度较慢，需要多次去噪迭代；\n\n依赖高质量演示数据；\n\n相比简单的行为克隆方法，计算开销更大。\n\n\n\n6. 提到的其他问题6.1 Diffusion Policy和RL以及Imitational Learning是什么关系？在Robot Learning领域，机器人操作比较常用的两个路径是强化学习（Reinforcement Learning）和模仿学习（Imitation Learning），Diffusion policy并不与强化学习和模仿学习冲突，它可以应用于两者。该方法是一种策略逻辑，适用于输入图像并输出相应动作的情境。在论文中，我们使用了模仿学习，即由人类遥控机器人执行动作，收集数据，并通过学习将其转化为策略。这种学习过程通过遥控机器人完成一系列动作开始，然后将其迁移到机器人身上。输入数据包括过去几帧的图像，而输出涉及对未来动作的预测。\n很多强化学习的人，他们使用强化学习在模拟器中生成大量数据。在这个过程中，为了加速训练，RL policy的输入不是图片，而是一些低维度的底层状态信息。但是由于这些状态信息在现实环境里是无法获得的，因此这个RL policy不能直接用于驱动机器人。这个时候，他们会把RL policy生成的数据用于训练一个图片作为输入的模仿学习策略，这被称为蒸馏。在这种情况下，模仿的对象并非人类，而是一个强化学习“代理”（Agent）。这也是这种方法的应用之一。\n6.2 操作（Manipulation）和移动（Locomotion）的训练有什么不同？RL在移动有更好的效果，Sim2Real Gap的问题相对好解决；但在操作，RL存在最大的问题是Sim2Real Gap没法很好的解决。对于操控而言，需要考虑的因素较多，其中一个关键区别是在机器人操作中除了需要考虑机器人本身的物理特性，同时还要适应复杂多变的环境和被操作物体。操控涉及与各种各样的物体进行交互，每个物体都具有独特的物理特性，如重心、摩擦力和动力学。这些在模拟器中难以准确模拟，即便能够模拟，精度通常较低，速度较慢。相比之下，对于locomotion，外界环境大多可以视为一个刚体，物理特性基本可以忽略。这使得可以花费更多时间来建立机器人本体的精确物理模型，以及设计更复杂的物理引擎。这是为什么RL更适合Locomotion，而对有物理机器人部署Manipulation没有那么好的效果。\n6.3 Diffusion Policy目前会存在什么问题，未来有哪些工作？目前最大的问题不是Policy本身，而是数据。训练数据对于机器人执行特定任务至关重要。尽管我们已经积累了一些关于遥控机器人执行任务的数据，但要将其部署到实际应用中，比如设计一个家用机器人来洗碗，就需要更大规模、更丰富多样的数据集，类似于ChatGPT的规模。确保机器人能够在各种家庭环境中表现出足够的稳健性。目前，最大的挑战在于如何有效地收集大量、多样化的数据。这是下一步研究的关键，通过数据收集和训练，期望能够解决当前面临的问题，同时也认识到可能会有新的挑战随着数据规模的增加而浮现。\n\n7. References：知乎\n宋舒然RSS 2023报告\nPrinceton Robotics - Russ Tedrake - Dexterous Manipulation with Diffusion Policies\nCheng Chi Github\nDiffusion Policy Project Page\n","categories":["Robots"],"tags":["Diffusion Policy"]},{"title":"Mujoco仿真环境下的机器人训练","url":"/2025/08/13/Mujoco%E4%BB%BF%E7%9C%9F%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%AE%AD%E7%BB%83/","content":"1. 引言在机器人学与强化学习研究中，仿真环境的选择直接影响训练效率与最终性能。MuJoCo（Multi-Joint dynamics with Contact） 作为一款高精度、多关节动力学仿真引擎，凭借其对刚体动力学、关节约束和接触碰撞的高效模拟能力，已成为机器人控制与策略学习领域的重要工具。相比其他物理引擎（如 Bullet、ODE、PhysX），MuJoCo 在以下几个方面具备显著优势：\n\n高保真动力学模拟：基于精确的解析力学求解器，能稳定处理复杂关节与接触约束。\n实时渲染与低延迟计算：可在高帧率下运行，支持在线调试与人机交互。\n灵活的模型定义：使用 XML 描述机器人结构，便于参数化设计与批量生成模型。\n与机器学习框架的无缝集成：可直接对接 Gymnasium、dm_control、Stable-Baselines3 等库，快速构建训练管线。\n\n在实际机器人研究中，MuJoCo 既可以作为算法验证平台，也能作为仿真—现实迁移（Sim2Real）的关键环节。通过在虚拟环境中大规模并行训练，研究者能够在节省硬件资源与时间成本的同时，快速迭代策略设计，并在必要时将策略迁移至实体机器人进行验证。\n本系列内容将围绕 “MuJoCo 仿真环境下的机器人训练” 展开，涵盖从模型搭建、环境封装，到强化学习训练与性能评估的完整流程。目标是为读者提供一套可复现的技术路线，使其能够在自己的研究或工程项目中高效利用 MuJoCo 完成机器人训练任务。\n\n2. MuJoCo 基础（基于 robosuite）2.1 robosuite 简介robosuite 是由斯坦福大学与英伟达联合开发的基于 MuJoCo 的机器人仿真框架，提供了大量预定义的机器人模型（Franka Panda、UR5e、Sawyer 等）与任务场景（如 Lift、Stack、NutAssembly），大幅降低了机器人强化学习任务的建模成本。其主要特性包括：\n\n多机器人支持：可同时模拟多个机械臂、手爪等执行机构\n高层 API：直接调用任务接口，而无需手写 MuJoCo XML\n丰富的传感器信息：关节状态、末端位姿、摄像机图像等\n与 RL 框架兼容：可快速接入 Gymnasium、Stable-Baselines3 等\n\n2.2 安装与环境配置1.安装 MuJoCo（建议使用官方 2.3+ 版本）：pip install mujoco\n\n或者手动下载 MuJoCo 官方版本，配置 MUJOCO_HOME 环境变量。\n2.安装 robosuite：pip install robosuite\n3.可选依赖（用于渲染和 RL 集成）：pip install opencv-python gymnasium stable-baselines3\n\n\n⚠ 注意：robosuite 需要 MuJoCo 运行时支持，因此在 Linux&#x2F;Mac 下要确保系统可访问 GPU（如果开启硬件加速渲染）。\n\n2.3 快速运行示例以下代码演示如何在 robosuite 中运行一个机械臂的抓取任务，并可视化仿真过程：\nimport robosuite as suitefrom robosuite.controllers import load_controller_config# 加载控制器配置（可选：OSC_POSE / JOINT_POSITION / JOINT_VELOCITY）controller_config = load_controller_config(default_controller=&quot;OSC_POSE&quot;)# 创建任务环境env = suite.make(env_name=&quot;Lift&quot;,               # 提升物体任务robots=&quot;Panda&quot;,                # 使用 Franka Panda 机械臂controller_configs=controller_config,has_renderer=True,             # 开启渲染窗口has_offscreen_renderer=False,  # 不启用离屏渲染use_camera_obs=False,          # 不使用摄像机图像control_freq=20                # 控制频率（Hz）)# 重置环境obs = env.reset()# 模拟运行for _ in range(1000):action = env.action_space.sample()  # 随机动作obs, reward, done, info = env.step(action)env.render()env.close()\n运行后，你将看到 Panda 机械臂随机动作尝试抓取方块的实时仿真窗口。\n2.4 常用 API 简介suite.make(env_name, robots, …)：创建环境\n\nenv.reset()：重置环境，返回初始观测\n\nenv.step(action)：执行动作，返回 (obs, reward, done, info)\n\nenv.render()：渲染环境（需要 has_renderer&#x3D;True）\n\ncontroller_configs：定义机械臂的控制模式（笛卡尔位置、关节速度等）\n\n\n\n3. 机器人建模（robosuite 下的 XML 扩展与定制场景）虽然 robosuite 提供了大量内置的机器人与任务，但在研究或工程项目中，我们常常需要 修改机械臂结构、添加新物体、或者定制交互环境。这部分工作依赖 MuJoCo 的 MJCF（XML）文件，robosuite 在运行时会加载这些 XML，因此我们可以通过继承和修改 XML 实现扩展。\n3.1 robosuite 模型文件结构robosuite 的模型位于：\nrobosuite/models/├── assets/           # 材质、纹理、几何体├── grippers/         # 夹爪模型├── robots/           # 机械臂模型├── objects/          # 场景中可交互的物体└── tasks/            # 任务 XML 组合文件\n例如 Panda 机械臂的 XML 文件路径：\nrobosuite/models/robots/panda/robot.xml\n3.2 添加自定义物体我们可以新建一个 my_block.xml，定义一个 0.05 m 边长的立方体，带有摩擦系数与质量参数：\n&lt;mujoco&gt;    &lt;asset&gt;        &lt;!-- 材质 --&gt;        &lt;texture type=&quot;cube&quot; name=&quot;block_tex&quot; width=&quot;512&quot; height=&quot;512&quot; rgb1=&quot;0.8 0.2 0.2&quot; rgb2=&quot;0.8 0.2 0.2&quot;/&gt;        &lt;material name=&quot;block_mat&quot; texture=&quot;block_tex&quot; specular=&quot;0.5&quot; shininess=&quot;0.5&quot;/&gt;    &lt;/asset&gt;    &lt;worldbody&gt;        &lt;body name=&quot;custom_block&quot; pos=&quot;0 0 0.025&quot;&gt;            &lt;geom name=&quot;block_geom&quot; type=&quot;box&quot; size=&quot;0.025 0.025 0.025&quot;                  material=&quot;block_mat&quot; density=&quot;1000&quot;                  friction=&quot;0.8 0.005 0.0001&quot; /&gt;        &lt;/body&gt;    &lt;/worldbody&gt;&lt;/mujoco&gt;\n\n关键参数：\n\nsize：几何体半尺寸（0.025 表示 5cm 边长立方体）\ndensity：密度（单位 kg&#x2F;m³）\nfriction：三个值分别为滑动摩擦、滚动摩擦、扭转摩擦\n\n\n3.3 将物体加入任务场景以 Lift 任务为例，原任务会加载一个默认的方块，我们可以将其替换为 my_block.xml。任务 XML 通常位于：\nrobosuite/models/tasks/Lift.xml\n在 &lt;worldbody&gt; 内找到物体定义，替换为：\n&lt;include file=&quot;../objects/my_block.xml&quot;/&gt;\n这样任务就会加载我们定义的立方体。\n3.4 在 Python 中加载自定义任务robosuite 允许你通过 自定义任务文件路径 来运行修改后的 XML 场景。\nimport robosuite as suitefrom robosuite.controllers import load_controller_configcontroller_config = load_controller_config(default_controller=&quot;OSC_POSE&quot;)env = suite.make(    env_name=&quot;Lift&quot;,               # 使用 Lift 任务逻辑    robots=&quot;Panda&quot;,                # Panda 机械臂    controller_configs=controller_config,    has_renderer=True,    use_camera_obs=False,    task_kwargs=&#123;&quot;model_xml_path&quot;: &quot;models/tasks/Lift.xml&quot;&#125;  # 指定修改后的任务 XML)obs = env.reset()for _ in range(500):    action = env.action_space.sample()    obs, reward, done, info = env.step(action)    env.render()env.close()\n这样，Panda 机械臂就会在仿真环境中尝试抓取我们自定义的红色立方体。\n3.5 进阶：自定义机器人如果需要修改机械臂的 关节长度、末端工具、自由度，可以直接复制并修改：\nrobosuite/models/robots/panda/robot.xml\n常用修改包括：\n\n调整  限制（range）\n\n更换夹爪模型（修改 ）\n\n添加传感器（ 节点）\n\n\n\n4. 训练流程：robosuite + robomimic 协同4.1 robomimic 简介robomimic 是由 NVIDIA 研究团队开发的 机器人模仿学习（Imitation Learning, IL） 框架，支持：\n\n行为克隆（BC）、基于强化学习的模仿（BC-RL）、生成式模仿（GAIL）\n\n与 robosuite 无缝集成（直接加载任务与机器人模型）\n\n统一的数据采集、回放与策略训练 API\n\n支持 多模态输入（关节状态 + 摄像机图像 + 力传感）\n\n\n在协同训练中，robosuite 负责提供 高保真物理仿真环境，robomimic 负责 采集数据、训练策略、执行评估。\n4.2 安装依赖# 安装 robosuitepip install robosuite# 安装 robomimic（建议源码安装以便修改配置）git clone https://github.com/ARISE-Initiative/robomimic.gitcd robomimicpip install -e .\n\n⚠ robomimic 需要 PyTorch &gt;&#x3D; 1.9，建议在虚拟环境中安装，避免和其他深度学习项目冲突。\n\n4.3 数据采集（使用 robosuite 环境）robomimic 提供了 collect_demos.py 脚本，可以直接从 robosuite 任务中采集专家演示数据。例如采集 Panda 机械臂 Lift 任务 的 50 条演示：\npython robomimic/scripts/collect_demos.py \\    --env robosuite \\    --env-name Lift \\    --robots Panda \\    --dataset-path datasets/lift/lift_demo.hdf5 \\    --num-demos 50 \\    --use-gui\n运行时会弹出 robosuite 渲染窗口，用户可以用键盘或 3D 鼠标控制机械臂完成任务，数据将保存到 .hdf5 文件中（包含观测、动作、奖励等信息）。\n4.4 策略训练（模仿学习 + 强化学习）以下示例展示如何用 robomimic 的 BC-RNN（行为克隆 + 循环网络）算法训练 Lift 任务策略：\npython robomimic/scripts/train.py \\    --config configs/bc_rnn.json \\    --dataset datasets/lift/lift_demo.hdf5 \\    --output-dir results/lift_bc_rnn\n配置文件 bc_rnn.json 中可以指定：\n\nmodel: 网络结构（MLP &#x2F; CNN &#x2F; LSTM）\n\nobs_modality: 使用的观测类型（状态 &#x2F; 图像）\n\ntrain: 训练轮数、批大小、学习率\n\nalgo: 算法类型（BC、BC-RNN、BC-RL、HBC 等）\n\n\n4.5 协同训练流程（IL + RL）robomimic 支持先用模仿学习初始化策略，再用 RL 微调。例如：\n1. 阶段 1（IL）：使用 BC 在专家演示上训练策略\n2. 阶段 2（RL）：将策略加载到 PPO &#x2F; SAC 继续训练，提高泛化能力\n示例流程：\n# 阶段 1：BC 训练python train.py \\    --config configs/bc.json \\    --dataset datasets/lift/lift_demo.hdf5 \\    --output-dir results/lift_bc# 阶段 2：SAC 微调python train.py \\    --config configs/sac.json \\    --dataset datasets/lift/lift_demo.hdf5 \\    --init-policy results/lift_bc/model_epoch_50.pth \\    --output-dir results/lift_bc_sac\n4.6 策略评估训练完成后，可使用 robomimic 的 rollout_policy.py 在 robosuite 环境中测试策略表现：\npython robomimic/scripts/rollout_policy.py \\    --dataset datasets/lift/lift_demo.hdf5 \\    --policy results/lift_bc_sac/model_epoch_100.pth \\    --video-path videos/lift_test.mp4\n4.7 协同训练优势\n数据高效：IL 阶段快速收敛，减少 RL 纯探索带来的低效\n\n策略稳定：BC 初始化降低 RL 收敛方差\n\n可扩展：支持多任务、多机器人，并可加入视觉模态\n\n\n","categories":["机器人"],"tags":["仿真环境","Robomimic & Robosuite"]},{"title":"My first blog","url":"/2025/06/02/My-first-blog/","content":"我的家乡——宁夏我的家乡宁夏，位于中国西北部，这里不仅有壮美的自然风光，还有深厚的历史文化底蕴，是一个充满魅力的地方。\n一、自然风光贺兰山贺兰山是宁夏的天然屏障，山脉巍峨，景色壮观。山间云雾缭绕，仿佛仙境一般。每到秋季，山上的树叶变得五彩斑斓，美不胜收。\n\n沙漠与绿洲宁夏的沙湖景区是沙漠与绿洲的完美结合。在这里，你可以看到一望无际的沙漠，也可以欣赏到湖水清澈、芦苇摇曳的绿洲美景，这种独特的自然景观令人陶醉。\n\n二、人文景观西夏王陵西夏王陵是西夏王朝的皇家墓地，具有极高的历史和文化价值。这里保存了许多古老的建筑和文物，见证了西夏王朝的兴衰。\n\n镇北堡西部影城镇北堡西部影城是中国著名的影视拍摄基地，许多著名的电影和电视剧都在这里取景。这里充满了浓郁的西部风情，仿佛带你穿越到电影中的世界。\n\n三、美食手抓羊肉手抓羊肉是宁夏的传统美食，以其肉质鲜嫩、味道醇厚而闻名。每到饭点，街头巷尾的餐馆都会飘出手抓羊肉的香味，让人垂涎欲滴。\n\n清真小吃宁夏的清真小吃种类繁多，如油香、馓子等。这些小吃不仅味道独特，而且制作工艺精湛，是宁夏美食文化的重要组成部分。\n\n我的家乡宁夏，既有壮美的自然风光，又有深厚的历史文化底蕴，还有令人难忘的美食。欢迎来到宁夏，亲身感受它的独特魅力！\n\n","categories":["个人"],"tags":["闲谈"]},{"title":"headless服务器渲染问题","url":"/2025/08/13/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%B8%B2%E6%9F%93%E9%97%AE%E9%A2%98/","content":"Headless 服务器渲染问题与配置指南引言在机器人仿真、自动化测试以及现代 Web 渲染中，越来越多的任务需要在 无头（headless）服务器 上运行。无头服务器没有显示器和 GUI 环境，这给依赖图形渲染的应用带来了挑战。例如，使用 OpenGL 的图形库或 Mujoco 仿真环境在无头服务器上直接运行常常会出现渲染失败或报错。本文将针对 OpenGL 配置问题、Mujoco 渲染失败 以及各种后端配置进行系统讲解。\n\n1. 安装基础依赖在 Linux 无头服务器上进行渲染，首先需要安装基础图形库和工具：\nsudo apt-get updatesudo apt-get install -y \\    mesa-utils-extra \\    libgl1-mesa-glx \\    libosmesa6 \\    libglfw3 \\    libglew-dev \\    xvfb\n\nmesa-utils-extra：提供 OpenGL 测试工具，如 glxinfo 和 glxgears\n\nlibgl1-mesa-glx &#x2F; libosmesa6：OpenGL 软件渲染库\n\nlibglfw3：GLFW 窗口和上下文管理\n\nlibglew-dev：OpenGL 扩展管理\n\nXvfb：虚拟帧缓冲，为无头环境提供显示支持\n\n\n安装完成后，可以测试 OpenGL 是否可用：\nglxinfo | grep &quot;OpenGL&quot;glxgears\n\n\n2. OpenGL 渲染后端选择在无头服务器中，OpenGL 可以通过不同后端实现渲染：\n\n\n\n后端\n特点\n使用场景\n\n\n\nEGL\n无需 X server，可直接与 GPU 或软件渲染接口交互\n推荐现代 headless GPU 环境\n\n\nGLX&#x2F;GLXW\n依赖 X server，可通过 Xvfb 提供虚拟显示\n兼容性好，适合测试或 CI&#x2F;CD\n\n\nOSMesa\n完全软件渲染，不依赖 GPU 或 X server\n没有 GPU 的纯 CPU 环境\n\n\n配置示例1. 软件渲染（OSMesa）：export LIBGL_ALWAYS_SOFTWARE=1export MUJOCO_GL=&quot;osmesa&quot;\n2. 使用 GLFW + Xvfb：export MUJOCO_GL=&quot;glfw&quot;xvfb-run -s &quot;-screen 0 1024x768x24&quot; python simulate_robot.py\n3. EGL 后端：export MUJOCO_GL=&quot;egl&quot;python simulate_robot.py\n\n注意：不同 OpenGL 后端可能对硬件和库依赖不同，可以根据服务器环境选择最优方案。\n\n3. Mujoco 渲染问题与解决方案Mujoco 在无头服务器上渲染失败的常见原因：\n\n无法创建 GL 上下文\n\nGPU 驱动或库缺失\n\n默认渲染后端不兼容\n\n\n推荐配置# 使用 GLFW 后端（兼容性最好）export MUJOCO_GL=&quot;glfw&quot;# 如果没有 GPU，可使用 OSMesa 软件渲染export MUJOCO_GL=&quot;osmesa&quot;# 启动虚拟显示（可选）xvfb-run -s &quot;-screen 0 1024x768x24&quot; python simulate_robot.py\n4. 总结与最佳实践\n安装完整依赖：mesa-utils-extra、Xvfb、GLFW、OSMesa 等。\n\n选择合适 OpenGL 后端：\n\n\n\nGPU 存在：EGL 或 GLFW\n\nGPU 不存在：OSMesa\n\n\n\n虚拟帧缓冲：Xvfb 可解决依赖显示的问题。\n\n环境变量配置：MUJOCO_GL、LIBGL_ALWAYS_SOFTWARE 保证渲染稳定。\n\nCI&#x2F;CD 与自动化测试：headless 渲染可以用于自动截图、仿真和验证。\n\n\n通过以上配置，可以在任何无头服务器上稳定运行 OpenGL 程序和 Mujoco 仿真，实现自动化渲染和测试。\n","categories":["服务器"],"tags":["服务器配置","Linux服务器"]},{"title":"每日计划","url":"/2025/08/04/%E6%AF%8F%E6%97%A5%E8%AE%A1%E5%88%92/","content":"📅 暑假每日计划（2025年8月4日起-）\n 🕗 早上八点起床\n 📘 学习英语六级（1小时）\n 📐 考研数学一和建模学习（1小时）\n 📡 学习《信号与系统》（1小时）\n 🌐 网站学习内容总结\n ✂️ 剪辑视频\n 🚗 调试车辆\n 📄 专利&#x2F;软著申请准备\n 📚 论文撰写\n\n","categories":["学习"],"tags":["计划"]},{"url":"/2025/08/30/%E7%94%B5%E8%B5%9B%E4%B8%89%E4%BA%BA%E7%89%88/","content":"🎯 E173首次电赛之旅｜2025 TI 杯比赛记录“初心大于胜负，成长大于输赢”，这是我们在备战和参与 2025 年“TI杯”全国大学生电子设计竞赛省赛过程中最深刻的体会。作为团队，我们首次完整经历了从校赛选拔到省赛冲刺的全过程。本次竞赛不仅是对我们专业知识和实践能力的综合考验，更是一次宝贵的团队协作与自我突破之旅。本文将对本次比赛进行全面总结，旨在梳理经验教训，为未来的学习和竞赛打下坚实基础。\n\n“初心大于胜负，成长大于输赢“，“立足培养、重在参与、鼓励探索、追求卓越”——这是我在一次又一次的比赛中学到的弥足珍贵的经验。\n\n\n🚗 校赛：从零开始，首次上阵\n校赛复盘：从零起步的探索\n\n1.任务与方案本次校赛的题目是复现 2023 年智能小车送药题。我们团队三人，分别负责了不同的技术环节：\n\n硬件设计与制作： 主要负责小车主控板的 PCB 绘制、打样及硬件调试。\n\n嵌入式软件开发： 负责 MSPM0 主控的底层驱动、电机控制和运动算法。\n\n计算机视觉： 负责基于 MaixCam 视觉 SOC 的图像识别和目标追踪。\n\n\n2.经验与教训\n\n硬件设计： 首次尝试快速 PCB 设计和打样，取得了成功，这为我们后续的硬件开发积累了宝贵经验。然而，由于对 MSPM0 主控板的数据手册理解不深，未预留电机编码器接口，导致小车控制系统存在开环缺陷，影响了定位精度。教训：在硬件设计初期，必须深入研究核心器件的数据手册，并充分考虑系统未来可能的升级和扩展需求。\n\n\n团队协作： 三人配合默契，通宵达旦进行联合调试，最终顺利通过校赛。这次经历让我们建立了基本的协作流程和信任基础。这次比赛从校赛开始，三人一起共同备战，复现2023年智能小车送药题，这次比赛尝试了快速绘制pcb板，打样并测试使用。\n\n很顺利地板子绘制很成功。但是由于不清楚MSPM0板的详细数据，没有为小车电机预留编码器接口，整个系统存在开环的缺陷，导致小车行进准确度很低。\n摄像头视觉采用maixcam很强的一款视觉SOC系统。在三人配合熬夜调车下，校赛顺利通过。附上一张帅气小车照：\n\n  \n\n\n\n  \n\n\n\n⏱️ 省赛：四天三夜，冲刺蜕变1.任务与过程\n省赛题目为“小车自瞄系统”。四天三夜的比赛中，我们采取了“分工协作，并行推进”的策略：\n\n第一天： 机械搭建与基础调试。一名队员负责小车机械主体和云台的组装；另外两名队员同时进行视觉循迹和黑框识别算法的开发与联调。\n\n第二天： 结构升级与通信调试。我们对机械结构进行了优化，并搭建了车载云台。通过蓝牙消息通讯，我们成功建立了三对数据通信链路，确保了主控、云台和视觉系统之间的数据流畅交换。\n\n第三天： 核心功能实现。在稳定通信的基础上，我们实现了小车的精准自瞄功能，完成了基础部分的任务。\n\n第四天： 系统优化与创新尝试。我们针对小车的基础能力进行了密集测试，并尝试了发挥部分的任务，以期获得更高的得分。\n\n\n2.经验与教训\n技术积累与应急学习： 面对新题目，我们展现了强大的应变能力，迅速学习并应用了所需的知识。这种“现学现用”的能力是比赛中的关键。\n\n系统级思考的缺失： 在机械结构设计上，我们未能考虑到导电滑环，导致云台旋转受限。在视觉方案上，缺少对图像逆透视处理的考虑，影响了远距离目标的识别精度。教训：一个完整的系统需要多学科知识的融合。在设计初期，必须进行全面的系统架构分析，预见潜在的技术瓶颈。\n\n\n时间管理与任务优先级： 四天时间紧迫，我们虽然完成了基础任务，但在发挥部分由于时间不足未能充分展现。教训：未来的比赛中，应制定更详尽的每日计划，并严格执行，确保核心功能稳定后，再投入时间进行优化和创新。在2025年7月30日到8月2日四天三夜的调试中，成长了许多，有什么不懂的知识就去现搞，不拖延，有计划的完成。\n\n比赛题目是小车自瞄系统：\n\n第一天搭车，修车老师傅上线，整个车搭建的同时视觉识别方案也在进行，第一天就完成了循迹任务以及黑框识别。\n第二天小车机械结构进一步升级，同时车载云台也搭建使用。采用蓝牙消息通讯，一共三对数据通讯线路，顺利配通。\n第三天已经开始喵靶很精准了，基础问的全部算是完成了。\n第四天加紧测试小车基础能力以及发挥部分的尝试。\n\n在四号晚上的紧张封箱中，赛程迎来了尾声。来一张心血作品照吧：\n\n  \n\n\n\n  \n\n\n\n✅ 稳定发挥！整车迎来最终的测评，小车很争气，在比赛中发挥正常，出色的完赛了。\n但是高手总是有的，遗憾才是比赛的主旋律吧。这次比赛机械结构没有考虑到导电滑环，摄像头没有考虑到逆透视效果，都造成了或多或少的遗憾。\n可是我坚信，宝贵的教训会让我们在以后的工作学习与生活中更加心思缜密：\n\n工程师思维，极客精神，攀峰精神······这才是比赛交给我们的吧。\n\n\n🔚 未来的路，继续自信发光通过这次电赛，我们团队收获的不仅是一辆能够出色完成任务的小车，更宝贵的是“工程师思维”和“攀峰精神”的洗礼。从最初的“调车小白”到如今能够独当一面的“调车老手”，我们每一个人都获得了显著成长。\n我们坚信，血的教训会让我们在未来的学习和工作中更加心思缜密。未来的路上，我们将：\n\n深化专业知识： 针对本次比赛中暴露出的短板（如控制系统理论、图像处理算法等），进行系统的学习和训练。\n\n加强实践能力： 积极参与各类项目和竞赛，将理论知识转化为实际的工程能力。\n\n提升团队协作效率： 探索更高效的协作工具和沟通方式，在未来的比赛中实现更紧密的配合。\n\n\n本次电赛是一次重要的里程碑，它标志着我们正式踏上了各自专业领域的探索之路。我们将带着这份宝贵的经验，继续自信发光，在所选择的道路上砥砺前行。\n"},{"title":"电赛记录","url":"/2025/08/04/%E7%94%B5%E8%B5%9B/","content":"🎯 我的首次电赛之旅｜2025 TI 杯省赛记2025年这一次电赛，是我们第一次参加的一场电赛，偶数年省赛奇数年国赛，这次的参与可谓是每一步都来得很不容易，而人也只有在困难面前顶着压力带着希望向前，其中的成长与经历，来的比任何奖项都要宝贵。\n\n“初心大于胜负，成长大于输赢“，“立足培养、重在参与、鼓励探索、追求卓越”——这是我在一次又一次的比赛中学到的弥足珍贵的经验。\n\n\n🚗 校赛：从零开始，首次上阵这次比赛从校赛开始，三人一起共同备战，复现2023年智能小车送药题，这次比赛我尝试了快速绘制pcb板，打样并测试使用。\n很顺利地板子绘制很成功，这也开启了我学习PCB设计的道路。但是由于不清楚MSPM0板的详细数据，没有为小车电机预留编码器接口，整个系统存在开环的缺陷，导致小车行进准确度很低。\n摄像头视觉采用maixcam很强的一款视觉SOC系统。在三人配合熬夜调车下，校赛顺利通过。附上一张帅气小车照：\n\n  \n\n\n\n  \n\n\n\n⏱️ 省赛：四天三夜，冲刺蜕变在2025年7月30日到8月2日四天三夜的调试中，成长了许多，有什么不懂的知识就去现搞，不拖延，有计划的完成。\n比赛题目是小车自瞄系统：\n\n第一天搭车，修车老师傅上线，整个车搭建的同时视觉识别方案也在进行，第一天就完成了循迹任务以及黑框识别。\n第二天小车机械结构进一步升级，同时车载云台也搭建使用。采用蓝牙消息通讯，一共三对数据通讯线路，顺利配通。\n第三天已经开始喵靶很精准了，基础问的全部算是完成了。\n第四天加紧测试小车基础能力以及发挥部分的尝试。\n\n在四号晚上的紧张封箱中，赛程迎来了尾声。来一张心血作品照吧：\n\n  \n\n\n\n  \n\n\n\n✅ 第五台小车，稳定发挥！整车迎来最终的测评，这是我造的第5台小车啦，小车很争气，每次都在比赛中发挥正常，出色的完赛了。\n但是高手总是有的，遗憾才是比赛的主旋律吧。这次比赛机械结构没有考虑到导电滑环，摄像头没有考虑到逆透视效果，都造成了或多或少的遗憾。\n可是我坚信，血的教训会让我们在以后的工作学习与生活中更加心思缜密：\n\n工程师思维，极客精神，攀峰精神······这才是比赛交给我们的吧。\n\n\n🧠 从“调车小白”到“调车老手”各种比赛兴奋中进行，泪水汗水遗憾自豪中结束，我也渐进成长为一个调车老手啦。\n很感慨，从高中入坑，大学进入RM战队赢下校内赛第一以来，自己调过无数次的车，熬过数不清的夜，睡在赛道上的经历历久弥新。\n智能车比赛自己从板子焊接开始到数千次的pid调试，自己学会了很多东西。每次从19教1楼走过，大脑仿佛暂停在之前调车的场景中。🔚 未来的路，继续自信发光很高兴自己是不断进步的，以此为我在电子信息工程专业的开端，我要在我选择的道路上继续走下去，继续自信发光的走下去。\n","categories":["比赛"],"tags":["经验"]}]